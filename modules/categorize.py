import json, re
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain_core.exceptions import OutputParserException
from wordcloud import WordCloud




# 1. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ + í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ 
keyword_schema = ResponseSchema(name='keywords', description='ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸')
keyword_parser = StructuredOutputParser.from_response_schemas([keyword_schema])
keyword_prompt = ChatPromptTemplate.from_template("""
    ë‹¹ì‹ ì€ ì •ì„±ì  ì‘ë‹µ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    ì•„ë˜ ì‚¬ìš©ì ì‘ë‹µ ëª©ë¡ì—ì„œ **í•µì‹¬ í‚¤ì›Œë“œ 3~5ê°œ**ë¥¼ ì‹ë³„í•˜ì„¸ìš”.
    - ìœ ì‚¬ í‘œí˜„ì€ ëŒ€í‘œ ë‹¨ì–´ë¡œ í†µì¼ (ì˜ˆ: 'ì»¤ë®¤ë‹ˆì¼€ì´ì…˜' â†’ 'ì†Œí†µ')
    - í‚¤ì›Œë“œëŠ” í•œêµ­ì–´ë¡œ ì¶”ì¶œ, ë‹¨ìˆœ ëª…ì‚¬ë³´ë‹¤ëŠ” ì¸ì‚¬ì´íŠ¸ ë„ì¶œì— ìœ ì˜ë¯¸í•œ ì§§ì€ êµ¬ë¬¸ì„ ìš°ì„  ì‚¬ìš©(ì˜ˆ: ì±…ì„ê°, ì†Œí†µ ì¤‘ì‹¬ ë¦¬ë”ì‹­, ì—…ë¬´ ì²˜ë¦¬ ì‹ ì†ì„±)
    - ë¶ˆí•„ìš”í•œ ì„¤ëª… ì—†ì´ JSONìœ¼ë¡œë§Œ ì‘ë‹µ

    ì‘ë‹µ ëª©ë¡:
    {texts}

    JSON ì¶œë ¥ ì˜ˆì‹œ:
    {{ "keywords": ["ì†Œí†µ", "ì±…ì„ê°", "ë¬¸ì œí•´ê²°"] }}
    """)

category_schema = ResponseSchema(name='categorized', description='í‚¤ì›Œë“œë³„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘')
category_parser = StructuredOutputParser.from_response_schemas([category_schema])
category_prompt = ChatPromptTemplate.from_template("""
                                                   
    ì•„ë˜ í‚¤ì›Œë“œë¥¼ ì£¼ì œë³„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    {keywords}
    JSON ì˜ˆì‹œ: [{{"keyword": "ì†Œí†µ", "category": "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜"}}]
    
    ì§€ì¹¨:
    - ì¹´í…Œê³ ë¦¬ëŠ” 'ì»¤ë®¤ë‹ˆì¼€ì´ì…˜', 'ì—…ë¬´íƒœë„', 'ì—­ëŸ‰', 'ì œë„ ë° í™˜ê²½', 'ê¸°íƒ€' 5ê°œë¡œ ì§€ì •í•¨
    - 'ì»¤ë®¤ë‹ˆì¼€ì´ì…˜'ì˜ ì£¼ìš” ì‚¬ë¡€ëŠ” 'ì†Œí†µ, í˜‘ì—…, ë¦¬ë”ì‹­, ì¡°ì§ë¬¸í™” ë“±'ì„
    - 'ì—…ë¬´íƒœë„'ì˜ ì£¼ìš” ì‚¬ë¡€ëŠ” 'ì±…ì„ê°, ì„±ì‹¤, ì—´ì •, ì ê·¹ ë“±'ì„
    - 'ì—­ëŸ‰'ì˜ ì£¼ìš” ì‚¬ë¡€ëŠ” 'í•´ê²°, ì „ë¬¸ì„±, ëŠ¥ë ¥, ì´í•´ë„ ë“±'ì„
    - 'ì œë„ ë° í™˜ê²½'ì˜ ì£¼ìš” ì‚¬ë¡€ëŠ” 'ë³µì§€, ì‹œìŠ¤í…œ, ê·¼ë¬´í™˜ê²½, ì¡°ì§ë¬¸í™”, êµìœ¡ ìš´ì˜, ì›Œë¼ë°¸ ë“±'ì„
    - 'ê¸°íƒ€'ëŠ” ìœ„ ë„¤ ê°€ì§€ì— ëª…í™•íˆ ë¶„ë¥˜ë˜ì§€ ì•ŠëŠ” ì˜ê²¬, ì œì•ˆ, ë‹¨ìˆœ ê°ì • í‘œí˜„, ëª¨í˜¸í•œ ì‘ë‹µ ë“±ì„ í¬í•¨í•¨
    - JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    - ë¶ˆí•„ìš”í•œ ì„¤ëª… ì—†ì´ JSONë§Œ ì‘ë‹µ

    í‚¤ì›Œë“œ ëª©ë¡:
    {keywords}

    ì¶œë ¥ ì˜ˆì‹œ:
    [
      {{ "keyword": "ì†Œí†µ", "category": "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜}},
      {{ "keyword": "ì±…ì„ê°", "category": "ì—…ë¬´íƒœë„" }}
    ]
    
""")

# ğŸ”¹ 2. í‚¤ì›Œë“œ ì¶”ì¶œ
def process_batch(batch, llm):
    messages = keyword_prompt.format_messages(texts=batch)
    response = llm.invoke(messages)
    raw = response.content

    try:
        return keyword_parser.parse(raw)["keywords"]
    except OutputParserException:
        match = re.search(r"\{.*\}", raw, flags=re.DOTALL)
        if match:
            try:
                parsed = json.loads(match.group(0))
                return parsed.get("keywords", [])
            except:
                return [] 
        return []

def extract_keywords_parallel(texts, llm, chunk_size=5, max_workers=4):
    all_keywords = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_batch, texts[i:i+chunk_size], llm)
                   for i in range(0, len(texts), chunk_size)]
        for f in as_completed(futures):
            all_keywords.extend(f.result())
    return all_keywords

#3. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
def categorize_keywords_batch(keywords, llm, batch_size=50):
    categorized = []
    for i in range(0, len(keywords), batch_size):
        batch = keywords[i:i+batch_size]
        prompt_text = category_prompt.format_messages(
            keywords=json.dumps(batch, ensure_ascii=False))[0].content
        
        
        response = llm([HumanMessage(content=prompt_text)])
        raw = response.content

        try:
            parsed = category_parser.parse(raw)
            categorized.extend(parsed)
        except:
            match = re.search(r"\[.*\]", raw, flags=re.DOTALL)
            if match:
                try:
                    categorized.extend(json.loads(match.group(0)))
                except:
                    continue
    return categorized


# 4. í†µí•© í•¨ìˆ˜ 
def run_keyword_analysis(texts, llm):
    keywords = extract_keywords_parallel(texts, llm)
    unique_keywords = sorted(set(keywords))

    parsed_cat = categorize_keywords_batch(unique_keywords, llm)
    categorized = {item["keyword"]: item["category"] for item in parsed_cat}

    df_kw = pd.DataFrame(keywords, columns=["keyword"])
    df_kw["category"] = df_kw["keyword"].map(categorized).fillna("ê¸°íƒ€")
    freq = df_kw.groupby(["keyword", "category"]).size().reset_index(name="count")

    return {
        "freq_df": freq,
        "keywords": keywords,
        "categorized": categorized
    }


    return freq, categorized

# 5. ì›Œë“œí´ë¼ìš°ë“œ 
def generate_wordcloud_from_freq(freq_df):
    
    # ë¹ˆ ì›Œë“œí´ë¼ìš°ë“œ ë°˜í™˜
    if freq_df.empty or 'keyword' not in freq_df.columns or 'count' not in freq_df.columns: 
        print("âš ï¸ freq_df is empty or missing required columns.")
        return None  
    
    freq_dict = pd.Series(freq_df['count'].values, index=freq_df['keyword']).to_dict() #
    wc = WordCloud(width=800, height=400, background_color='white', font_path='malgun.ttf')
    return wc.generate_from_frequencies(freq_dict)
